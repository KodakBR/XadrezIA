# Chess AI with Monte Carlo Tree Search and Deep Q-Learning

Este Ã© um projeto de Xadrez com InteligÃªncia Artificial utilizando uma combinaÃ§Ã£o de **Monte Carlo Tree Search (MCTS)** e **Deep Q-Learning (DQN)**. O objetivo Ã© criar agentes que possam jogar xadrez em nÃ­vel competitivo, melhorando suas habilidades atravÃ©s de simulaÃ§Ãµes e aprendizado por reforÃ§o.

## ğŸš€ Tecnologias Utilizadas

- **Python**: Linguagem principal do projeto.
- **Pygame**: Biblioteca usada para a interface grÃ¡fica do jogo.
- **TensorFlow**: Framework utilizado para a criaÃ§Ã£o e treinamento da rede neural.
- **Keras**: API de alto nÃ­vel para construÃ§Ã£o da rede neural.
- **Chess**: Biblioteca para a manipulaÃ§Ã£o e controle das regras do xadrez.

## ğŸ§  InteligÃªncia Artificial

### Deep Q-Learning (DQN)
O DQN Ã© utilizado para treinar dois agentes: **Ana** (peÃ§as brancas) e **Pedro** (peÃ§as pretas). Cada agente utiliza uma rede neural para prever a melhor jogada com base no estado atual do tabuleiro.

### Monte Carlo Tree Search (MCTS)
O MCTS Ã© usado para realizar simulaÃ§Ãµes em segundo plano e ajudar os agentes a escolherem os melhores movimentos possÃ­veis. Para cada jogada do jogo principal, vÃ¡rias simulaÃ§Ãµes sÃ£o realizadas para determinar o movimento com a maior pontuaÃ§Ã£o.

## âš™ï¸ Como Funciona

1. **Tabuleiro Principal**: O jogo principal Ã© exibido na tela e jogado entre os dois agentes treinados.
2. **SimulaÃ§Ãµes em Background**: Para cada jogada no tabuleiro principal, o agente realiza vÃ¡rias simulaÃ§Ãµes em segundo plano para determinar o melhor movimento.
3. **Treinamento ContÃ­nuo**: A cada jogada, os agentes sÃ£o treinados com base nos resultados das simulaÃ§Ãµes, e os modelos sÃ£o salvos para uso posterior.

## ğŸ›  Como Executar

1. **Clone o RepositÃ³rio**
    ```bash
    git clone https://github.com/KodakBR/XadrezIA)
    ```

2. **Instale as DependÃªncias**
    ```bash
    pip install -r requirements.txt
    ```

3. **Execute o Jogo**
    ```bash
    python xadrez.py
    ```

4. **Escolha o NÃºmero de SimulaÃ§Ãµes**
    Ao iniciar o programa, vocÃª serÃ¡ solicitado a inserir o nÃºmero de simulaÃ§Ãµes para cada jogada. Essas simulaÃ§Ãµes sÃ£o realizadas para ambos os agentes em segundo plano.

## ğŸ–¥ Interface GrÃ¡fica

A interface grÃ¡fica exibe apenas o tabuleiro principal onde o jogo acontece. As simulaÃ§Ãµes em background sÃ£o realizadas sem exibiÃ§Ã£o grÃ¡fica para otimizar o desempenho.

### Controles
- O tabuleiro principal Ã© atualizado automaticamente conforme os agentes fazem suas jogadas.
- O resultado de cada jogada e o movimento realizado sÃ£o exibidos no terminal.

## ğŸ“‚ Estrutura do Projeto

```plaintext
.
â”œâ”€â”€ README.md           # DocumentaÃ§Ã£o do projeto
â”œâ”€â”€ xadrez.py           # CÃ³digo principal do jogo e inteligÃªncia artificial
â”œâ”€â”€ requirements.txt    # DependÃªncias do projeto
â”œâ”€â”€ training.log        # Log do treinamento dos agentes
â””â”€â”€ xadrez ico/         # DiretÃ³rio contendo as imagens das peÃ§as

ğŸ“ˆ Resultados
Modelos Treinados: Os modelos dos agentes sÃ£o salvos em ana_model.h5 e pedro_model.h5, sendo continuamente aprimorados com o tempo.
Logs de Treinamento: Os resultados e o progresso do treinamento sÃ£o registrados no arquivo training.log.

ğŸ¤ ContribuiÃ§Ãµes
Sinta-se Ã  vontade para contribuir com melhorias no cÃ³digo, relatÃ³rios de bugs ou novas funcionalidades atravÃ©s de pull requests.

ğŸ“ CrÃ©ditos
Este projeto foi desenvolvido por Kein Soares.
